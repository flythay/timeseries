{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn-contrib-py-earth\n",
      "  Using cached sklearn-contrib-py-earth-0.1.0.tar.gz (1.0 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-contrib-py-earth) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn-contrib-py-earth) (1.4.1.post1)\n",
      "Requirement already satisfied: six in c:\\users\\thayse\\appdata\\roaming\\python\\python311\\site-packages (from sklearn-contrib-py-earth) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\thayse\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->sklearn-contrib-py-earth) (2.2.0)\n",
      "Building wheels for collected packages: sklearn-contrib-py-earth\n",
      "  Building wheel for sklearn-contrib-py-earth (setup.py): started\n",
      "  Building wheel for sklearn-contrib-py-earth (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for sklearn-contrib-py-earth\n",
      "Failed to build sklearn-contrib-py-earth\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [116 lines of output]\n",
      "      c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\setuptools\\dist.py:476: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2024-Sep-26, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\setuptools\\__init__.py:81: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Requirements should be satisfied by a PEP 517 installer.\n",
      "              If you are using pip, you can try `pip install --use-pep517`.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist.fetch_build_eggs(dist.setup_requires)\n",
      "      c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\setuptools\\dist.py:476: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2024-Sep-26, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\earth.py -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\export.py -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_version.py -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\__init__.py -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      creating build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\testing_utils.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_earth.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_export.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_forward.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_knot_search.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_pruning.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_qr.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_util.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\__init__.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      creating build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\base.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_basis.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_constant.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_hinge.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_linear.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_missingness.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\test_smoothed_hinge.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      copying pyearth\\test\\basis\\__init__.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\basis\n",
      "      creating build\\lib.win-amd64-cpython-311\\pyearth\\test\\record\n",
      "      copying pyearth\\test\\record\\test_forward_pass.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\record\n",
      "      copying pyearth\\test\\record\\test_pruning_pass.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\record\n",
      "      copying pyearth\\test\\record\\__init__.py -> build\\lib.win-amd64-cpython-311\\pyearth\\test\\record\n",
      "      running egg_info\n",
      "      writing sklearn_contrib_py_earth.egg-info\\PKG-INFO\n",
      "      writing dependency_links to sklearn_contrib_py_earth.egg-info\\dependency_links.txt\n",
      "      writing requirements to sklearn_contrib_py_earth.egg-info\\requires.txt\n",
      "      writing top-level names to sklearn_contrib_py_earth.egg-info\\top_level.txt\n",
      "      reading manifest file 'sklearn_contrib_py_earth.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching 'pyearth\\test\\pathological_data'\n",
      "      adding license file 'LICENSE.txt'\n",
      "      writing manifest file 'sklearn_contrib_py_earth.egg-info\\SOURCES.txt'\n",
      "      copying pyearth\\_basis.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_basis.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_forward.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_forward.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_knot_search.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_knot_search.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_pruning.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_pruning.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_qr.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_qr.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_record.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_record.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_types.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_types.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_util.c -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\_util.pxd -> build\\lib.win-amd64-cpython-311\\pyearth\n",
      "      copying pyearth\\test\\earth_linvars_regress.txt -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\earth_regress.txt -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\earth_regress_missing_data.txt -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\earth_regress_smooth.txt -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\forward_regress.txt -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      copying pyearth\\test\\test_data.csv -> build\\lib.win-amd64-cpython-311\\pyearth\\test\n",
      "      UPDATING build\\lib.win-amd64-cpython-311\\pyearth/_version.py\n",
      "      set build\\lib.win-amd64-cpython-311\\pyearth/_version.py to '0.1.0'\n",
      "      running build_ext\n",
      "      building 'pyearth._util' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\pyearth\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\Thayse\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\include -Ic:\\ProgramData\\Anaconda3\\include -Ic:\\ProgramData\\Anaconda3\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /Tcpyearth/_util.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\pyearth/_util.obj\n",
      "      _util.c\n",
      "      pyearth/_util.c(172): fatal error C1083: N\\xc6o \\x82 poss\\xa1vel abrir arquivo incluir: 'longintrepr.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for sklearn-contrib-py-earth\n",
      "ERROR: Could not build wheels for sklearn-contrib-py-earth, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn-contrib-py-earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyearth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyearth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Earth\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Configurações de gráficos e warnings\u001b[39;00m\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfivethirtyeight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyearth'"
     ]
    }
   ],
   "source": [
    "# Importações\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyearth import Earth\n",
    "\n",
    "# Configurações de gráficos e warnings\n",
    "plt.style.use('fivethirtyeight')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Função para carregar dados de múltiplos arquivos Excel\n",
    "def carregar_dados(pasta, amostra=False):\n",
    "    dados_finais = []\n",
    "    for root, _, files in os.walk(pasta):\n",
    "        for file in files:\n",
    "            caminho = os.path.join(root, file)\n",
    "            print(f\"Carregando: {caminho}\")\n",
    "            f = pd.ExcelFile(caminho)\n",
    "            for sheet in f.sheet_names:\n",
    "                df = f.parse(sheet)\n",
    "                df['pasta'] = file\n",
    "                if amostra:\n",
    "                    df = df.sample(frac=0.25, random_state=42)\n",
    "                dados_finais.append(df)\n",
    "    return pd.concat(dados_finais, ignore_index=True)\n",
    "\n",
    "# Função para pré-processar os dados\n",
    "def preprocessar_dados(df):\n",
    "    if df['windspeed'].dtype == object:\n",
    "        df['windspeed'] = df['windspeed'].str.replace(',', '.').astype(float)\n",
    "    df['velocidade_vento'] = df['windspeed']\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    Q1 = df['velocidade_vento'].quantile(0.25)\n",
    "    Q3 = df['velocidade_vento'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[(df['velocidade_vento'] >= (Q1 - 1.5 * IQR)) & (df['velocidade_vento'] <= (Q3 + 1.5 * IQR))]\n",
    "    df['velocidade_vento'] = df['velocidade_vento'].interpolate()\n",
    "    return df\n",
    "\n",
    "# Função auxiliar para garantir que os tamanhos de y_predito e y_verdadeiro sejam iguais\n",
    "def ajustar_tamanhos(y_predito, y_verdadeiro):\n",
    "    if len(y_predito) > len(y_verdadeiro):\n",
    "        y_predito = y_predito[-len(y_verdadeiro):]\n",
    "    elif len(y_verdadeiro) > len(y_predito):\n",
    "        y_verdadeiro = y_verdadeiro[-len(y_predito):]\n",
    "    return y_predito, y_verdadeiro\n",
    "\n",
    "# Função genérica para rodar modelos\n",
    "def rodar_modelos_paises(df, paises, modelo_func, n_splits=5, periodos=365):\n",
    "    resultados_final = Parallel(n_jobs=-1)(delayed(modelo_func)(df, pais, n_splits, periodos) for pais in paises)\n",
    "    return pd.DataFrame(resultados_final)\n",
    "\n",
    "# Funções para otimizar e rodar SARIMAX\n",
    "def otimizar_parametros_sarimax(trial, ts):\n",
    "    pdq = trial.suggest_categorical('pdq', [(0, 0, 0), (1, 1, 1), (1, 0, 1), (0, 1, 1), (1, 1, 0)])\n",
    "    seasonal_pdq = trial.suggest_categorical('seasonal_pdq', [(0, 0, 0, 12), (1, 1, 1, 12), (1, 0, 1, 12), (0, 1, 1, 12), (1, 1, 0, 12)])\n",
    "    try:\n",
    "        mod = SARIMAX(ts['y'], order=pdq, seasonal_order=seasonal_pdq, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        resultados = mod.fit(disp=False)\n",
    "        return resultados.aic\n",
    "    except Exception as e:\n",
    "        print(f\"Erro com {pdq}, {seasonal_pdq}: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "def rodar_sarimax_pais(df, pais, n_splits, periodos):\n",
    "    scaler = MinMaxScaler()\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    print(f\"Analisando país: {pais}\")\n",
    "    db = df[df['pasta'] == pais]\n",
    "    ts = db[['datetime', 'velocidade_vento']].rename(columns={\"datetime\": 'ds', \"velocidade_vento\": \"y\"})\n",
    "    ts['y'] = ts['y'].astype(float)\n",
    "    ts = ts.sort_values('ds').set_index('ds')\n",
    "    ts.index = pd.to_datetime(ts.index)\n",
    "    ts = ts.resample('M').mean().reset_index()\n",
    "    ts = ts.dropna()\n",
    "\n",
    "    ts_escalado = scaler.fit_transform(ts[['y']])\n",
    "    ts_escalado = pd.DataFrame(ts_escalado, index=ts.index, columns=['y'])\n",
    "\n",
    "    estudo = optuna.create_study(direction='minimize')\n",
    "    estudo.optimize(lambda trial: otimizar_parametros_sarimax(trial, ts_escalado), n_trials=50)\n",
    "    melhor_param = estudo.best_params\n",
    "\n",
    "    mod = SARIMAX(ts_escalado, order=melhor_param['pdq'], seasonal_order=melhor_param['seasonal_pdq'])\n",
    "    resultados = mod.fit()\n",
    "\n",
    "    predicao = resultados.get_forecast(steps=n_splits)\n",
    "    y_predito = predicao.predicted_mean\n",
    "    y_verdadeiro = ts_escalado['y'][-n_splits:]\n",
    "\n",
    "    y_predito, y_verdadeiro = ajustar_tamanhos(y_predito, y_verdadeiro)\n",
    "\n",
    "    mse_teste = mean_squared_error(y_verdadeiro, y_predito)\n",
    "    mae_teste = mean_absolute_error(y_verdadeiro, y_predito)\n",
    "    r2_teste = r2_score(y_verdadeiro, y_predito)\n",
    "\n",
    "    return {\n",
    "        'pais': pais,\n",
    "        'modelo': 'SARIMAX',\n",
    "        'MSE': mse_teste,\n",
    "        'MAE': mae_teste,\n",
    "        'R2': r2_teste\n",
    "    }\n",
    "\n",
    "# Funções para otimizar e rodar SVR\n",
    "def otimizar_parametros_svr(trial, ts, tss):\n",
    "    C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-4, 1e1)\n",
    "    epsilon = trial.suggest_loguniform('epsilon', 1e-3, 1e1)\n",
    "    svr = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "    mse_scores = []\n",
    "    for train_index, test_index in tss.split(ts):\n",
    "        X_treino, X_teste = ts.index[train_index], ts.index[test_index]\n",
    "        y_treino, y_teste = ts.iloc[train_index], ts.iloc[test_index]\n",
    "        svr.fit(np.array(X_treino).reshape(-1, 1), y_treino)\n",
    "        y_teste_predito = svr.predict(np.array(X_teste).reshape(-1, 1))\n",
    "        mse_scores.append(mean_squared_error(y_teste, y_teste_predito))\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "def rodar_svr_pais(df, pais, n_splits, periodos):\n",
    "    scaler = MinMaxScaler()\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    print(f\"Analisando país: {pais}\")\n",
    "    db = df[df['pasta'] == pais]\n",
    "    ts = db[['datetime', 'velocidade_vento']].rename(columns={\"datetime\": 'ds', \"velocidade_vento\": \"y\"})\n",
    "    ts['y'] = ts['y'].astype(float)\n",
    "    ts = ts.sort_values('ds').set_index('ds')\n",
    "    ts.index = pd.to_datetime(ts.index)\n",
    "    ts = ts.resample('M').mean().reset_index()\n",
    "    ts = ts.dropna()\n",
    "\n",
    "    ts_escalado = scaler.fit_transform(ts[['y']])\n",
    "    ts_escalado = pd.DataFrame(ts_escalado, index=ts.index, columns=['y'])\n",
    "\n",
    "    estudo = optuna.create_study(direction='minimize')\n",
    "    estudo.optimize(lambda trial: otimizar_parametros_svr(trial, ts_escalado, tss), n_trials=50)\n",
    "    melhor_svr = SVR(kernel='rbf', **estudo.best_params)\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for train_index, test_index in tss.split(ts_escalado):\n",
    "        X_treino, X_teste = ts.index[train_index], ts.index[test_index]\n",
    "        y_treino, y_teste = ts.iloc[train_index], ts.iloc[test_index]\n",
    "\n",
    "        melhor_svr.fit(np.array(X_treino).reshape(-1, 1), y_treino)\n",
    "\n",
    "        y_treino_predito = melhor_svr.predict(np.array(X_treino).reshape(-1, 1)).reshape(-1, 1)\n",
    "        y_teste_predito = melhor_svr.predict(np.array(X_teste).reshape(-1, 1)).reshape(-1, 1)\n",
    "\n",
    "        y_treino_predito = scaler.inverse_transform(y_treino_predito)\n",
    "        y_teste_predito = scaler.inverse_transform(y_teste_predito)\n",
    "        y_treino_orig = scaler.inverse_transform(y_treino.values.reshape(-1, 1))\n",
    "        y_teste_orig = scaler.inverse_transform(y_teste.values.reshape(-1, 1))\n",
    "\n",
    "        y_teste_predito, y_teste_orig = ajustar_tamanhos(y_teste_predito, y_teste_orig)\n",
    "\n",
    "        mse_teste = mean_squared_error(y_teste_orig, y_teste_predito)\n",
    "        mae_teste = mean_absolute_error(y_teste_orig, y_teste_predito)\n",
    "        r2_teste = r2_score(y_teste_orig, y_teste_predito)\n",
    "\n",
    "        resultados.append({\n",
    "            'pais': pais,\n",
    "            'modelo': 'SVR',\n",
    "            'MSE': mse_teste,\n",
    "            'MAE': mae_teste,\n",
    "            'R2': r2_teste\n",
    "        })\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Funções para otimizar e rodar Prophet\n",
    "def otimizar_parametros_prophet(trial, ts):\n",
    "    changepoint_prior_scale = trial.suggest_loguniform('changepoint_prior_scale', 0.001, 0.5)\n",
    "    seasonality_prior_scale = trial.suggest_loguniform('seasonality_prior_scale', 0.01, 10.0)\n",
    "    \n",
    "    modelo = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=changepoint_prior_scale,\n",
    "        seasonality_prior_scale=seasonality_prior_scale\n",
    "    )\n",
    "    modelo.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    modelo.fit(ts)\n",
    "\n",
    "    futuro = modelo.make_future_dataframe(periods=365, freq='M')\n",
    "    previsao = modelo.predict(futuro)\n",
    "    y_predito = previsao['yhat'][-len(ts):].values\n",
    "    y_verdadeiro = ts['y'].values\n",
    "\n",
    "    return mean_squared_error(y_verdadeiro, y_predito)\n",
    "\n",
    "def rodar_prophet_pais(df, pais, n_splits, periodos):\n",
    "    print(f\"Analisando país: {pais}\")\n",
    "    db = df[df['pasta'] == pais]\n",
    "    ts = db[['datetime', 'velocidade_vento']].rename(columns={\"datetime\": 'ds', \"velocidade_vento\": \"y\"})\n",
    "    ts['y'] = ts['y'].astype(float)\n",
    "    ts = ts.sort_values('ds').dropna()\n",
    "\n",
    "    estudo = optuna.create_study(direction='minimize')\n",
    "    estudo.optimize(lambda trial: otimizar_parametros_prophet(trial, ts), n_trials=50)\n",
    "    melhor_params = estudo.best_params\n",
    "\n",
    "    m = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=melhor_params['changepoint_prior_scale'],\n",
    "        seasonality_prior_scale=melhor_params['seasonality_prior_scale']\n",
    "    )\n",
    "    m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "    m.fit(ts)\n",
    "\n",
    "    futuro = m.make_future_dataframe(periods=periodos, freq='M')\n",
    "    previsao = m.predict(futuro)\n",
    "\n",
    "    y_predito = previsao['yhat'][-len(ts):].values\n",
    "    y_verdadeiro = ts['y'].values\n",
    "\n",
    "    y_predito, y_verdadeiro = ajustar_tamanhos(y_predito, y_verdadeiro)\n",
    "\n",
    "    mse_teste = mean_squared_error(y_verdadeiro, y_predito)\n",
    "    mae_teste = mean_absolute_error(y_verdadeiro, y_predito)\n",
    "    r2_teste = r2_score(y_verdadeiro, y_predito)\n",
    "\n",
    "    resultados_previsao = previsao[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "    print(resultados_previsao)\n",
    "\n",
    "    m.plot(previsao)\n",
    "    plt.title(f\"Previsões - {pais}\")\n",
    "    plt.show()\n",
    "\n",
    "    m.plot_components(previsao)\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'pais': pais,\n",
    "        'modelo': 'Prophet',\n",
    "        'MSE': mse_teste,\n",
    "        'MAE': mae_teste,\n",
    "        'R2': r2_teste\n",
    "    }\n",
    "\n",
    "# Classe para encapsular o modelo MARS\n",
    "class MARSModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = Earth(max_degree=self.degree)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Funções para otimizar e rodar MARS\n",
    "def otimizar_parametros_mars(trial, ts, tss):\n",
    "    degree = trial.suggest_int('degree', 1, 5)\n",
    "    mars_model = MARSModel(degree=degree)\n",
    "    \n",
    "    mse_scores = []\n",
    "    for train_index, test_index in tss.split(ts):\n",
    "        X_treino, X_teste = ts.index[train_index], ts.index[test_index]\n",
    "        y_treino, y_teste = ts.iloc[train_index], ts.iloc[test_index]\n",
    "        mars_model.fit(np.array(X_treino).reshape(-1, 1), y_treino)\n",
    "        y_teste_predito = mars_model.predict(np.array(X_teste).reshape(-1, 1))\n",
    "        mse_scores.append(mean_squared_error(y_teste, y_teste_predito))\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "def rodar_mars_pais(df, pais, n_splits, periodos):\n",
    "    scaler = MinMaxScaler()\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "    print(f\"Analisando país: {pais}\")\n",
    "    db = df[df['pasta'] == pais]\n",
    "    ts = db[['datetime', 'velocidade_vento']].rename(columns={\"datetime\": 'ds', \"velocidade_vento\": \"y\"})\n",
    "    ts['y'] = ts['y'].astype(float)\n",
    "    ts = ts.sort_values('ds').set_index('ds')\n",
    "    ts.index = pd.to_datetime(ts.index)\n",
    "    ts = ts.resample('M').mean().reset_index()\n",
    "    ts = ts.dropna()\n",
    "\n",
    "    ts_escalado = scaler.fit_transform(ts[['y']])\n",
    "    ts_escalado = pd.DataFrame(ts_escalado, index=ts.index, columns=['y'])\n",
    "\n",
    "    X = np.arange(len(ts_escalado)).reshape(-1, 1)\n",
    "    y = ts_escalado['y'].values\n",
    "\n",
    "    estudo = optuna.create_study(direction='minimize')\n",
    "    estudo.optimize(lambda trial: otimizar_parametros_mars(trial, ts_escalado, tss), n_trials=50)\n",
    "    melhor_mars = MARSModel(degree=estudo.best_params['degree'])\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for train_index, test_index in tss.split(ts_escalado):\n",
    "        X_treino, X_teste = X[train_index], X[test_index]\n",
    "        y_treino, y_teste = y[train_index], y[test_index]\n",
    "\n",
    "        melhor_mars.fit(X_treino, y_treino)\n",
    "\n",
    "        y_treino_predito = melhor_mars.predict(X_treino).reshape(-1, 1)\n",
    "        y_teste_predito = melhor_mars.predict(X_teste).reshape(-1, 1)\n",
    "\n",
    "        y_treino_predito = scaler.inverse_transform(y_treino_predito)\n",
    "        y_teste_predito = scaler.inverse_transform(y_teste_predito)\n",
    "        y_treino_orig = scaler.inverse_transform(y_treino.reshape(-1, 1))\n",
    "        y_teste_orig = scaler.inverse_transform(y_teste.reshape(-1, 1))\n",
    "\n",
    "        y_teste_predito, y_teste_orig = ajustar_tamanhos(y_teste_predito, y_teste_orig)\n",
    "\n",
    "        mse_teste = mean_squared_error(y_teste_orig, y_teste_predito)\n",
    "        mae_teste = mean_absolute_error(y_teste_orig, y_teste_predito)\n",
    "        r2_teste = r2_score(y_teste_orig, y_teste_predito)\n",
    "\n",
    "        resultados.append({\n",
    "            'pais': pais,\n",
    "            'modelo': 'MARS',\n",
    "            'MSE': mse_teste,\n",
    "            'MAE': mae_teste,\n",
    "            'R2': r2_teste\n",
    "        })\n",
    "\n",
    "    return resultados\n",
    "\n",
    "# Carregar e pré-processar os dados\n",
    "df = carregar_dados(pasta_dados)\n",
    "df = preprocessar_dados(df)\n",
    "\n",
    "# Definição dos países\n",
    "paises = df['pasta'].unique()\n",
    "\n",
    "# Executa a otimização e avaliação dos modelos\n",
    "resultado_sarimax = rodar_modelos_paises(df, paises, rodar_sarimax_pais)\n",
    "resultado_svr = rodar_modelos_paises(df, paises, rodar_svr_pais)\n",
    "resultado_prophet = rodar_modelos_paises(df, paises, rodar_prophet_pais, periodos=365)\n",
    "resultado_mars = rodar_modelos_paises(df, paises, rodar_mars_pais)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"Resultados SARIMAX:\")\n",
    "print(pd.DataFrame(resultado_sarimax))\n",
    "print(\"\\nResultados SVR:\")\n",
    "print(pd.DataFrame(resultado_svr))\n",
    "print(\"\\nResultados Prophet:\")\n",
    "print(pd.DataFrame(resultado_prophet))\n",
    "print(\"\\nResultados MARS:\")\n",
    "print(pd.DataFrame(resultado_mars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
